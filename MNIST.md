# 神经网络、激活函数、SoftMax、损失函数、交叉熵、梯度下降

Ref:https://www.jianshu.com/p/696bde1641d8

1. 神经网络：https://www.zhihu.com/question/22553761
2. 激活函数：每个神经元，在通过一系列计算后，得到了一个数值，怎么来判断应该输出什么呢？激活函数就是解决这个问题，你把值给我，我来判断怎么输出。所以一个神经网络，激活函数是非常重要的。
3. SoftMax：我们知道max(A,B)，是指A和B里哪个大就取哪个值，但我们有时候希望比较小的那个也有一定概率取到，怎么办呢？我们就按照两个值的大小，计算出概率，按照这个概率来取A或者B。比如A=9，B=1,那取A的概率是90%，取B的概率是10%。
4. 损失函数：损失函数是模型对数据拟合程度的反映，拟合得越好损失应该越小，拟合越差损失应该越大，然后我们根据损失函数的结果对模型进行调整。
5. 交叉熵：这个概念要解释的简单，那就不准确，如果要准确，那可能一千字都打不住。这里说一个简单但不一定准确的解释吧。比如，你想把乾坤大挪移练到第七层大圆满，你现在是第五层，那你还差两层，这个两层就是你和大圆满之间的距离。交叉熵通俗的讲就是现在的训练程度和圆满之间的距离，我们希望距离越小越好，所以交叉熵可以作为一个损失函数，来衡量和目标之间的距离。
6. 梯度下降：这个概念可以这样理解，我们要解决的问题是一座山，答案在山底，我们从山顶到山底的过程就是解决问题的过程。在山顶，想找到最快的下山的路。这个时候，我们的做法是什么呢？在每次选择道路的时候，选最陡的那条路。梯度是改变率或者斜度的另一个称呼，用数学的语言解释是导数。对于求损失函数最小值这样的问题，朝着梯度下降的方向走，就能找到最优值了。

